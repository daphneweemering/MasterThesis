---
title: Interim sample size reestimation for adequately powered series of N-of-1 trials
author:
- name: Daphne N. Weemering*
  num: a
address:
- num: a
  org: Department of Methodology and Statistics, Utrecht University, Utrecht, The Netherlands
corres: "*Corresponding author name, This is sample corresponding address. \\email{authorone@gmail.com}"
presentaddress: Padualaan 14, 3584 CH Utrecht, The Netherlands
authormark: Daphne N. Weemering
articletype: Research article
received: 2022-05-09
revised: 2022-05-09
accepted: 2022-05-09
abstract: "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut elit odio. Donec fermentum tellus neque, vitae fringilla orci pretium vitae. Fusce maximus finibus facilisis. Donec ut ullamcorper turpis. Donec ut porta ipsum. Nullam cursus mauris a sapien ornare pulvinar. Aenean malesuada molestie erat quis mattis. Praesent scelerisque posuere faucibus. Praesent nunc nulla, ullamcorper ut ullamcorper sed, molestie ut est. Donec consequat libero nisi, non semper velit vulputate et. Quisque eleifend tincidunt ligula, bibendum finibus massa cursus eget. Curabitur aliquet vehicula quam non pulvinar. Aliquam facilisis tortor nec purus finibus, sit amet elementum eros sodales. Ut porta porttitor vestibulum. Integer molestie, leo ut maximus aliquam, velit dui iaculis nibh, eget hendrerit purus risus sit amet dolor. Sed sed tincidunt ex. Curabitur imperdiet egestas tellus in iaculis. Maecenas ante neque, pretium vel nisl at, lobortis lacinia neque. In gravida elit vel volutpat imperdiet. Sed ut nulla arcu. Proin blandit interdum ex sit amet laoreet. Phasellus efficitur, sem hendrerit mattis dapibus, nunc tellus ornare nisi, nec eleifend enim nibh ac ipsum. Aenean tincidunt nisl sit amet facilisis faucibus. Donec odio erat, bibendum eu imperdiet sed, gravida luctus turpis."
keywords: N-of-1 trials; sample size reestimation; simulation study; statistical methods;
bibliography: bibfile.bib
output:
  bookdown::pdf_book:
    base_format: rticles::sim_article
header_includes: 
 - \usepackage{float}
 - \floatplacement{figure}{H!}
---

# Introduction 
Randomized controlled trials (RCTs) are considered the gold standard in determining treatment efficacy in healthcare. At first glance, these standard RCTs seem to earn their position as the randomization of patients into a parallel experimental and control condition works quite well in balancing factors that are not under experimental control, allowing for unbiased estimation of the population treatment effect. A drawback, however, is that these standard RCTs require a relatively large sample size to establish the effectiveness of treatment with sufficient power. In all instances it is desirable to limit the number of subjects to enter a medical study, as it exposes individuals to potentially inferior treatment. However, for the instances of finding the right intervention for patients with rare diseases, standard RCTs become unfeasible due to the relatively large sample size they require. As populations of patients with rare diseases only have a limited number of people that could enter such a experimental study, alternatives should be sought to obtain reliable estimates of the treatment effect. 

A clinical trial methodology that offers the possibility to reduce the number of subjects necessary to find a treatment effect with sufficient power, is the N-of-1 trial. The N-of-1 trial is a randomized controlled multiple crossover trial where a single patient repeatedly receives the experimental and control intervention in a random order, spread out over multiple cycles [@guyatt1986]. As the experiment is conducted within a single patient, the advantage of the N-of-1 trial is that a patient-specific treatment effect estimate is obtained. Often, clinical evidence that is generated by standard RCTs turns out to have poor generalization and is therefore to a limited amount applicable to patients in ordinary practices [@greenfield2007]. Also, treatments that are shown to be safe on average, may have a disbalance in risks and benefits to individual patients [@duan2013]. With an N-of-1 trial, these issues can be avoided by purely estimating the effect of treatment on a single patient. 

To obtain reliable results, N-of-1 trials should be performed under specific clinical circumstances. First of all, the disease under study should be long term and stable over time in order to avoid that the treatment differences will be obscured within and between cycles due to disease progression. Second, the intervention should not modify the course of the disease and should have a rapid on- and offset of biological action of the medication [@araujo2016; @nikles2011]. 

A single N-of-1 trial does not compare itself with a standard RCT, as the results of a single trial are specific to an individual patient, and hence, cannot be generalized to the population. However, when the results of several separate N-of-1 trials are combined it is possible to estimate the population treatment effect [@zucker1997]. Even though every patient is unique, there is also some similarity in the disease progression and the treatment response of patients. In the combined analysis of separate N-of-1 trials, both the magnitude of the treatment effect as well as the heterogeneity in treatment response are taken into account [@zucker1997]. The combined estimate of the separate trials forms the basis of knowledge about the treatment, whereas the estimate of the heterogeneity in treatment response allows for determining optimal treatment for every single patient by balancing the similarities and differences. As it is possible to estimate the population treatment effect within each subject, combined N-of-1 trials can serve as efficient alternatives for standard parallel RCTs. 

Combined N-of-1 trials, now referred to as series of N-of-1 trials, have been performed in, inter alia, studying the influence of mexiletine on nondystrophic myotonia [@stunnenberg2018], studying the effectiveness of methylphenidate on fatigue in patients with end-stage cancer [@mitchell2015], an for investigating the usefulness of sildenafil on Raynaus-Phenomenon patients [@roustit2018]. Reasons for choosing the N-of-1 trial methodology vary, in general but also specifically for these aforementioned studies. The latter study chose to conduct a series of N-of-1 trials due to the heterogeneity in treatment response that should be taken into account, whereas the first two studies chose for the N-of-1 trial methodology due to inability to achieve the required sample size for a standard RCT. 

Series of N-of-1 trials require a smaller sample size compared to standard two-arm RCTs, because every patient serves as its own control, creating the possibility to obtain multiple observations per individual [@araujo2016]. A prior sample size calculations are necessary to avoid under- or overpowering the study, for planning on allocating resources and for assessing the feasibility of the study. For these a priori sample size calculations, assumptions have to made with regard to unknown parameters in the model. The nuisance parameters (i.e., the stochastic model components) are often unknown before the start of the studies. Incorrect estimates for these nuisance parameters can lead to substantial over- or underpowering. Overpowering a study potentially exposes too many patients to inferior treatment, whereas underpowering a study increases the risk of committing an error of the second kind, where one is not able to find an effect of treatment when there exists one in the population. 

Sample size formulas have been derived for series of N-of-1 trials for both random and fixed effects models [@senn2019]. As the main objective of combining the results of separate N-of-1 trials is to make inferences to the population, random effects models are most appropriate and of interest here. In series of N-of-1 trials, the within- and between subject variance of the response to treatment are unknown at the start of the studies [@senn2019]. Sample size estimation in series of N-of-1 trials require estimates for these unknown parameters. Taking estimates of nuisance parameters from other studies can be unreliable because of differences in the study population, background conditions or study design [@zucker2002]. Furthermore, an estimate of the between-subject variance in treatment response is often not available because the kind of study to obtain these estimates is a trial incorporating such a component, such as a series of N-of-1 trials [@senn2016]. Series of N-of-1 trials are not (yet) that common, and even if similar series of N-of-1 trials exist, these kinds of estimates are usually not reported in the literature. 

An appealing strategy for conquering the problem of incorrect assumptions for unknown parameters in sample size calculations is the two-stage design. With this design, the required sample size is calculated by making reasonable assumptions for the unknown parameters. Then, data is obtained up until a predetermined interim point along the trial. The data that is obtained up until the interim point is used to estimate the parameters that were unknown prior to the studies. These estimates at interim are then used to reestimate the sample size, which can then be adjusted accordingly [@proschan2005]. This two-stage design, now referred to as interim sample size reestimation, can avoid a study to become under- or overpowered in the scenario where wrong assumptions are made with regard to unknown parameters in sample size calculations. A distinction can be made between interim sample size reestimation based on nuisance parameter estimates and based on treatment effect estimates [@proschan2009]. This thesis will not cover the latter approach, but rather focuses on interim sample size reestimation based on nuisance parameter estimates. 

A general concern with interim sample size reestimation, is the inflation of the type I error rate. Various studies have attempted to analytically calculate or control the type I error rate under various circumstances [@stein1945; @wittes1990; @kieser2000; @gao2008]. Under specific circumstances, this type I error rate becomes inflated, and this is not desirable. Investigating the influence of interim sample size reestimation for series of N-of-1 trials on the type I error rate is not the main objective of this thesis, but it will be assessed whether the type I error rate becomes inflated for specific scenarios considered here. 

The application of interim sample size reestimation has not yet been investigated in the context of series of N-of-1 trials and no specific guidelines have been established. Moreover, the minimally required sample size for interim sample size reestimation in series of N-of-1 trials has also not yet been established. With the use of simulation studies, this thesis aims at establishing guidelines for the minimally required sample size for sample size reestimation in series of N-of-1 trials, and to compare series of N-of-trials incorporating interim sample size reestimation with series of N-of-1 trials having a fixed sample size. Power and expected sample size are important measures of performance herein. Furthermore, the type I error rate between series of N-of-1 trials with and without the incorporation of interim sample size reestimation will be assessed. 

The remainder of this paper will be structured as follows: In section \@ref(modelsection), the model used for the simulation studies and the notation are discussed. In section \@ref(sampsizecalcs), sample size calculations for series of N-of-1 trials are discussed. In section \@ref(simulations) the setup of the simulation studies will be explained. Section \@ref(results) discusses the results of the simulations studies, and section \@ref(discussion) includes the conclusions that can be drawn based on the results, the limitations of this study and considerations for future research on this topic. 

# Model, assumptions and notation {#modelsection}
Simulation studies are performed to compare the reliability and efficacy of series of N-of-1 trials including sample size reestimation with series of N-of-1 trials without including the interim reestimation process of the sample size. For simulating the data, it is assumed that individuals receive treatment A and treatment B once within each cycle. The order of treatment administration within each cycle will be randomly determined. Furthermore, it is assumed that the outcome concerns a continuous measurement, that the disease under study is stable over time, and that carryover effects are absent because of a sufficient duration of the washout period. Lastly, it is assumed that there are no missing data. 

Previous research [@chen2014; @araujo2016; @senn2019] has shown that linear mixed models provide robust inferences for series of N-of-1 trial data. The so called summary measures approach [@araujo2016] uses a linear mixed model, only this model calculates the mean difference in outcome under treatment A minus the mean difference in outcome under treatment B. The unit of analysis under the summary measures approach becomes the patient instead of the cycle, which would have been the unit of analysis under a mixed effects model for the original observations. If the data is balanced, the summary measures approach under this model will lead to the same result as a mixed model [@senn2019]. Using this approach results in a simpler model:

\begin{equation} 
d_{ij} = \tau_i + \epsilon_{ij}
(\#eq:popmodel)
\end{equation}

In this model, $d_{ij}$ is the observed treatment difference for patient $i = 1,...,n$ in cycle $j = 1,...,k$), where treatment B is subtracted from treatment A (or vice versa, as long as it is consistently applied), $\tau_i \sim N(T, \psi^2)$, and $\epsilon_{ij} \sim N(0, 2\sigma^2)$. $\tau_i$ is the random treatment effect for patient $i$. This term has a common average $T$ and a variance $\psi^2$, which indicates how much the individual treatment effects vary from each other. $\epsilon_{ij}$ are random within-patient within-cycle disturbance terms. These disturbance terms are assumed to be i.i.d. (independent and identically distributed) both across cycles and across patients. The variance term of $\epsilon_{ij}$, $2\sigma^2$, is given to make this summary measures model compatible with a mixed model using the original observations [@senn2019]. 

## Further notation 
In section \ref{simulations} the simulations studies are explained in detail, and the notation used in that section will be clarified here. The nuisance parameter ($\psi^2$ and $\sigma^2$) values that are hypothesized (i.e. assumed) a priori are indicated with a subscript $h$: $\psi^2_h$ and $\sigma^2_h$. The nuisance parameters values that are used for data generation, and which may be considered the true values of these nuisance parameters, are indicated with a subscript $t$: $\psi^2_t$ and $\sigma^2_t$. The observed interim values for $\psi^2$ and $\sigma^2$ are indicated as $\psi^2_{obs}$ and $\sigma^2_{obs}$.For the sample size $n$, the initial sample size is indicated as $n_{init}$, the fraction of the initial sample size on which sample size reestimation is based is indicated as $n_{frac}$, and the reestimated, final sample size is indicated as $n_{final}$. 

# Sample size calculations in series of N-of-1 trials {#sampsizecalcs}
From the model in equation 1, the average treatment effect in the population and the corresponding variance could be derived [@senn2019], which are necessary for calculating the required sample size. Following Senn [@senn2019], the summary measures approach is used. First, the data can be reduced to an average treatment effect for each individual patient: 

\begin{equation} 
\bar{d_{i.}} = \sum_{j=1}^{k} \frac{d_{ij}}{k}
(\#eq:equation2)
\end{equation}

The mean over all the $n$ individual treatment differences ($\bar{d_{i.}}$) is then defined as:

\begin{equation} 
\hat{T} = \frac{\sum_{i=1}^{n} \sum_{j=1}^{k} d_{ij}}{nk}
(\#eq:equation3)
\end{equation}

$\bar{T}$ can be used to test the difference between two treatments (say treatment A and treatment B) at hand. This average over all the $n$ separate patients will have a variance of

\begin{equation} 
var(\hat{T}) = \frac{\psi^2 + 2\sigma^2 /k}{n}
(\#eq:equation4)
\end{equation}

Then, the variance at the level of the individual patient is equal to $var(\bar{d_{i.}}) = \psi^2 + 2\sigma^2 /k$. This variance, the variance of the $n$ summary measures $\bar{d_{i.}}$, has ($n-1$) degrees of freedom and can be used to calculate the sample size under hypothesized values for $\psi^2$ and $\sigma^2$ using a one sample $t$-test. The exact formula for the calculation of the sample size is based on a non-central $t$-distribution. Computation of the sample size therefore requires an iterative process and this can straightforwardly be done with the ```pwr.t.test``` function of the ```pwr``` package [@champely2018] in ```R``` [@Rmanual].

# Simulations {#simulations}
Simulation studies are conducted to compare the method of interim sample size reestimation in series of N-of-1 trials with a series of N-of-1 trials that do not include interim sample size reestimation for varying values of the nuisance parameters. Special guidelines have been established for planning and reporting simulation studies based on the so called "ADEMP" structure (Aim, Data-generating mechanism, Estimands, Methods, Performance measures) [@morris2019] and this section is written in accordance with this structure. 

## Aim 
The aims of the simulation studies are twofold First, by means of simulation studies the minimally required sample size for reliable reestimation of the sample size at interim is sought under various scenarios for the nuisance parameters. By comparing the reestimated sample size with the sample size under the true nuisance parameter values ($\psi^2_t$ and $\sigma^2_t$) statements about the reliability of the reestimation method can be made. Second, the aim is to compare series of N-of-1 trials incorporating interim sample size reestimation with series of N-of-1 trials having a fixed sample size on power and the type I error rate.

## Data-generating mechanism 
The linear mixed model provided in equation \@ref(eq:popmodel) will be used to simulate data for series of N-of-1 trials. As $\psi^2$ and $\sigma^2$ are unknown prior to the study, assumptions have to be made with regard to these parameters. The scenarios that are considered in the simulation study are shown in table \@ref(tab:table1). The data is generated under all possible combinations of $\psi^2_t = 0.5, 1, 2$ and $\sigma^2_t = 0.25, 0.5, 1$ ($3^2 = 9$ combinations of $\psi^2_t$ and $\sigma^2_t$). For each generated data set, all combinations of the scenarios where $\psi^2_h = 0.5, 1, 2$ and $\sigma^2_h = 0.25, 0.5, 1$ are considered (resulting in $3^4 = 81$ combinations of $\psi^2_t$, $\sigma^2_t$, $psi^2_h$, and $\sigma^2_h$). Lastly, for each of the $81$ scenarios so far discussed, a different fraction ($f = 0.25, 0.5, 0.75$) of the initially calculated sample size is taken to base the interim sample size reestimation on. In total, $3^5 = 243$ scenarios are considered. 

For the interpretation of the results in a subsequent section, labels are specified to the sizes of the variance parameters. For the between-subject variance, $\psi^2 = 0.5$ is considered small heterogeneity, $\psi^2 = 1$ moderate heterogeneity, and $\psi^2 = 2$ large heterogeneity. For the within-subject within cycle variance, $\sigma^2 = 0.25$ is considered small error, $\sigma^2 = 0.5$ moderate error, and $\sigma^2 = 1$ large error. 

\begin{table}[h] 
\begin{center}
\caption{Parameters in the two simulation studies}
\begin{tabular}{p{8cm}p{3.5cm}p{3.5cm}} 
\hline
Description & Constant parameters & Varied parameters \\
\hline 
\textbf{Design parameters} & & \\
Fraction of initial sample size & & $f = 0.25, 0.5, 0.75$ \\
Cycles per patient & $k = 3$ & \\
Power & $1 - \beta = 0.8$ & \\
Two-sided nominal significance level & $\alpha = 0.05$ & \\
Clinically relevant difference & & $\Delta = 0, 1$ \\
\textbf{Model parameters} & & \\
Within-patient within-cycle variance (data generation) & & $\sigma^2_t = 0.25, 0.5, 1$ \\
Variance of treatment effect (data generation)  & & $\psi^2_t = 0.5, 1, 2$ \\
Within-patient within-cycle variance (hypothesized) & & $\sigma^2_h = 0.25, 0.5, 1$ \\
Variance of treatment effect (hypothesized)  & & $\psi^2_h = 0.5, 1, 2$ \\
Average treatment effect & & $T = \Delta = 0, 1$  \\
\textbf{Simulation parameter} & & \\
Number of simulations & $N = 10000$ & \\
\hline 
\multicolumn{3}{l}{\textbf{Outcome parameters}} \\
Statistical power & \multicolumn{2}{p{8cm}}{Compare power of trial with sample size reestimation in simulation to target of 80\%}\\
Sample size & \multicolumn{2}{p{8cm}}{Compare sample size under reestimation with sample size under true parameter values} \\
Type I error rate & \multicolumn{2}{p{8cm}}{Compare type I error rate of trial with sample size reestimation in simulation to target of 5\%} \\
\hline 
\end{tabular}
\end{center}
\end{table}

In the first simulation study, the clinically relevant difference, $\Delta$, will be $1$ both for data generation and for calculating the sample size, allowing for estimation of the power of series of N-of-1 trials incorporating interim sample size reestimation. In the second simulation study, $\Delta$ will be set to $0$ for generating the data and assumed $1$ for calculating the sample size in order to estimate the type I error rate. As $\alpha$ is the probability of falsely rejecting the null hypothesis when it is true [@neyman1933], setting $\Delta$ for data generation equal to zero (which it would be under the null hypothesis) and subsequently calculating the sample size assuming $\Delta = 1$ allows for the estimation of the type I error rate. 

## Estimands 
The estimand (the parameter to be estimated) that is of interest in the simulation studies, is the average treatment effect in the population $T$.

## Methods 
Linear mixed models will be fitted to the simulated data using the ```lme4``` package [@lme4] in ```R``` [@Rmanual]. In fitting the linear mixed models, restricted maximum likelihood (REML) will be applied to estimate the variance parameters, as this approach, to the contrary of "regular" maximum likelihood (ML), produces unbiased estimates for the variance components in the model for smaller sample sizes [@corbeil1976]. 

The initial sample size, $n_{init}$, will be calculated as the sample size required to achieve 80\% power under $\psi^2_h$, $\sigma^2_h$, the clinically relevant difference $\Delta$, and the two-sided significance level $\alpha = 0.05$. Sample size calculations will be performed using the ```pwr``` package [@champely2018] in ```R```. Equation 3 will be applied to calculate the standard deviation for the standardized effect size needed for the ```pwr.t.test``` function. A one-sample $t$-test is used to test the treatment difference. The sample size will be reevaluated after $f*n$ subjects (where $f$ represents the fraction of the initial sample size) using $\psi^2_{obs}$ and $\sigma^2_{obs}$. Eventually, the results of the trials including interim sample size reestimation will be compared with trials having a fixed sample size. 

## Performance measures
The performance of interim sample size reestimation will be evaluated by means of statistical power, the expected sample size and the type I error rate. Power is defined as $\frac{p \leq 0.05}{N}$, where $p$ is the $p$-value of the test statistic and $N$ is the number of iterations in the simulation. The same formula for the power can be used to estimate the type I error rate when for the generation of the data $\Delta = 0$. The power will be compared to a target of 80\%, the type I error rate will be compared with the target of 5\%. The sample size under reestimation (and under a fixed sample size) will be compared with the sample size under $\psi^2_t$ and $\sigma^2_t$. If interim sample size reestimation works properly, the reestimated sample size should be close to the sample size under the true model.  

# Results {#results}
The simulation studies allowed for the investigation of the question regarding the minimally required sample size for reliable reestimation of the sample size at interim, and for the comparison on the power and type I error rate between series of N-of-1 trials that incorporate interim sample size reestimation with series of N-of-1 trials that have a fixed sample size set in advance. 

## Sample size {#expsamplesize}
The average reestimated sample size under $\psi_h^2$ and $\sigma_h^2$ is approximately equal to the sample size under the true values for data generation, $\psi_t^2$ and $\sigma_t^2$, as displayed in figure \@ref(fig:fig1), which displays the reestimated sample size under different combinations of $\psi_h^2$, $\sigma_h^2$, $\psi_t^2$, $\sigma_t^2$ and $f$. Table S1 in the supplementary materials includes the corresponding table to this figure. 
```{r, include = FALSE}
setwd('/Users/daphneweemering/Google Drive/UU/Thesis/Thesis/Figures')
load('results_sampsize_plots.RData')
```

```{r, echo = FALSE, message = FALSE, fig.cap = 'Reestimated sample sizes in simulated series of N-of-1 trials under different data, hypothesized values for the nuisance parameters, and fractions of the initial sample size. The dotted line indicates the sample size under the true values for the nuisance parameters.\\label{fig:fig1}', fig.width = 7, fig.height = 6.2, fig.align = 'center'}
# Load libraries
library(ggplot2)
library(dplyr)

ggplot(data = results, mapping = aes(x = fraction_cat, y = reestimsampsize, color = hyp_psi_sigma)) +
  facet_grid(true_psi_cat ~ true_sigma_cat) +
  geom_point(position = position_dodge(0.7), size = 3, shape = 21) +
  geom_errorbar(aes(ymin = reestimsampsize - sd_final_sampsize,
                    ymax = reestimsampsize + sd_final_sampsize), 
                width = 0.5, position = position_dodge(0.7)) +
  xlab('Fraction of the initial sample size (f)') +
  ylab('Reestimated sample size') +
  theme(axis.text.x = element_text(size = 8, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 8),
        strip.text.x = element_text(size = 8),
        axis.title = element_text(size = 9.5),
        legend.title = element_text(size = 9.5),
        legend.text = element_text(size = 8)) +
  labs(color = expression(paste(sigma[h]^2, ' and ', psi[h]^2))) +
  geom_hline(data = results, aes(yintercept = hline), linetype = "dotted")
```

Even though the reestimated sample size is approximately equal to the sample size under the true scenario for most of the hypothesized scenarios, the variation in the reestimated sample size is quite large in some cases. From figure \@ref(fig:fig1), it quickly becomes clear that the larger the fraction of the initial sample size on which interim sample size reestimation is based, the smaller the variance in reestimated sample sizes. This makes sense, as using a larger fraction of the initial sample size to estimate the nuisance parameters $\psi_{obs}$ and $\sigma_{obs}$ increases the precision of these estimates and hence also the reestimated sample size. 

Additionally, the size of the two nuisance parameters also seems to influence the variability in reestimated sample sizes. The size of $\psi_t^2$, the between-person variance, seems to have a greater influence on the variability in reestimated sample sizes than $\sigma_t^2$, the within-person within cycle variance. If there exists a lot of interperson heterogeneity in treatment effectiveness in the population, then this influences the effect size, and thus also the sample size, more than when there would be a lot of intraperson heterogeneity in the treatment effect. 

Furthermore, the results show greater variability in reestimated sample size when $\psi_h^2$ is relatively small (in this case when $\psi_h^2 = 0.5$ compared to when $\sigma_h^2$ is small. This greater variability in $\psi_h^2$ occurs regardless of the true underlying values for the nuisance parameters. However, the larger the discrepancy between the true and hypothesized values for $\psi^2$, the larger the variation in reestimated sample sizes. Noteworthy are the results that show that even when the hypothesized and the true values for the between-person variance are equal to each other, it still occurs that the variance in reestimated sample sizes is largest when $\psi_h^2$ (and thus also $\psi_t^2$) is small. 

## Power {#power}

```{r, include = FALSE}
# Set working directory and load in the data
setwd('/Users/daphneweemering/Google Drive/UU/Thesis/Thesis/Figures')
load('results_power_plots.RData')
```

```{r, echo = FALSE, fig.cap = 'Power in simulated series of N-of-1 trials under different data and hypothesized values for the nuisance parameters. The x-axis indicates the different combinations of the nuisance parameters for data generation ($\\psi^2$ and $\\sigma^2$, respectively). The dotted line indicates a power of 0.8.\\label{fig:fig2}', fig.width = 6.8, fig.height = 6.2, fig.align = 'center'}
# Load library
library(ggplot2)

# Make the figure for the power
ggplot(data = results, mapping = aes(x = true_psi_sigma, y = power, shape = hyp_sigma_cat,
                                     color = hyp_psi_cat,)) +
  geom_jitter(position = position_jitter(0.18), size = 2, stroke = 0.8) +
  facet_grid(fraction_cat ~ .) +
  xlab(expression(paste('Values for ', sigma[t]^2, ' and ', psi[t]^2))) +
  ylab('Power') +
  geom_hline(yintercept = 0.8, linetype = "dotted") +
  theme(axis.text.x = element_text(size = 8, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 8),
        strip.text.x = element_text(size = 8),
        axis.title = element_text(size = 9.5),
        legend.title = element_text(size = 9.5),
        legend.text = element_text(size = 8)) +
  labs(shape = expression(sigma[h]^2), color = expression(psi[h]^2)) +
  scale_shape_manual(values = c(1, 2, 0)) +
  guides(shape = guide_legend(order = 1),
         color = guide_legend(order = 2))
```

Despite the fact that variability can be quite high for some values of the nuisance parameters overall power for these cases might still be adequate. Figure \@ref(fig:fig2) depicts the power of all the different scenarios in the simulation study. These results for the power can also be found in table S2 in the supplementary materials. As the values for $\psi_h^2$ and $\sigma_h^2$ showed to have different effects on the reestimated sample size, these are separated by means of shape and color in the figure. First, the scenarios for the combinations of the true and hypothesized nuisance parameters where $f$ is equal to 0.25 are considered. Evidently, these scenarios showed the greatest variability in reestimated sample sizes. Figure \@ref(fig:fig2) shows that a part of these scenarios also result in an underpowered study. Especially the scenarios where the hypothesized between-person variance is small (0.5) and the between-person in the population is large (2) lead to a lack of statistical power. For most of these cases the reduction in power remains limited, but the cases where $\psi_h^2 = 0.5$, $\sigma_h^2 = 0.25$ and where $\psi_t^2 = 2)$ lead to a power of 0.7. Here again the limited influence of the true value for the within-person variance becomes clear. 

Apart from the cases where underpowering occurs, there are also some cases where an excess of statistical power in evident. For the scenarios where $\psi_t^2 = 0.5$, all the combinations for the hypothesized nuisance parameters lead to overpowered studies. Especially the scenarios where the true value for the between-subject variance is small and the hypothesized values for this parameter are large will lead to an excess of power. The profusion of power becomes even more evident when a larger fraction of the initial sample size is taken to base interim sample size reestimation on. When $f = 0.75$, the combination of a small $\psi_t^2$ and a large $\psi_h^2$ will even lead to studies with a power close to 1. By taking the 75\% of the initial sample size, almost all the scenarios considered here will lead to overpowered studies, with the exception of the scenarios where $\psi_h^2 = 0.5$. Even though overpowering a study might be not as bad as underpowering a study, as the result in case of an underpowered study might be less reliable, overpowering results in quite a waste of resources and is therefore also not very desirable.

The results under the scenarios where $f = 0.5$ are a bit more moderate then the results of the scenarios discussed before. Again, the cases where $\psi_h^2 = 0.5$ and where $\psi_t^2 = 2$ result in slight underpowered studies, and the scenarios where $\psi_h^2$ is large result in quite overpowered studies, with the exception of the scenario where the true value for the between-person variance is also large. However, excess power stays present for most scenarios. 

A last noteworthy point on the power in these simulations is the fact that overpowering also occurs when the hypothesized and true values of the nuisance parameters are equal to each other. Even when the "guess" for the nuisance parameters with right at the start of the studies, an excess of power is inevitable, at least for the scenarios discussed here. 

## Type I error rate {#alpharate}
```{r, include = FALSE}
# Set working directory and load in the data
setwd('/Users/daphneweemering/Google Drive/UU/Thesis/Thesis/Figures')
load('results_alpha_plots.RData')
```

```{r, echo = FALSE, fig.cap = 'Type I error rate in simulated series of N-of-1 trials under different data and hypothesized values for the nuisance parameters. The x-axis indicates the different combinations of the nuisance parameters for data generation ($\\psi^2$ and $\\sigma^2$, respectively). The dotted line indicates the nominal $\\alpha$ level of 0.05. \\label{fig:fig3}', fig.width = 6.8, fig.height = 6.2, fig.align = 'center'}
# Load library
library(ggplot2)

# Make the figure for the type I error rate
ggplot(data = results, mapping = aes(x = true_psi_sigma, y = power_or_alpha, color = hyp_psi_cat,
                                     shape = hyp_sigma_cat)) +
  geom_jitter(position = position_jitter(0.18), size = 2, stroke = 0.8) +
  facet_grid(fraction_cat ~ .) +
  xlab(expression(paste('Values for ', sigma[t]^2, ' and ', psi[t]^2))) +
  ylab('Type I error rate') +
  geom_hline(yintercept = 0.05, linetype = "dotted") +
  theme(axis.text.x = element_text(size = 8, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 8),
        strip.text.x = element_text(size = 8),
        axis.title = element_text(size = 9.5),
        legend.title = element_text(size = 9.5),
        legend.text = element_text(size = 8)) +
  labs(shape = expression(sigma[h]^2), color = expression(psi[h]^2)) +
  scale_shape_manual(values = c(1, 2, 0)) +
  guides(shape = guide_legend(order = 1),
         color = guide_legend(order = 2))

```

Finally, the effect of interim sample size reestimation on the type I error rate was examined. In section \@ref(power), the effect of interim sample size reestimation on the statistical power was discusses. As the type I error rate and power are interrelated with each other, it is interesting to see if the excess of power that was discovered in the previous section will also translate in an increase in the rejection region or if it stays constant. 

Figure \@ref(fig:fig3) depicts the type I error rate for the different scenarios of the true and hypothesized nuisance parameters per fraction of the initial sample size on which reestimation was based. These results can also be found in tabular format in table S3 in the supplementary materials. First, the scenarios where $f = 0.25$. Here, the size of $\sigma_t^2$ in combination with the size of $\psi_h^2$ seem of most influence on the type I error rate. For smaller values of $\sigma_t^2$, the type I error rate becomes inflated for the smaller values of $\psi_h^2$, resulting in type I error rates around 0.07. Larger values of $\psi_h^2$ under $\sigma_t^2 = 0.25$ remain closer to the nominal level of $\alpha$. When $\sigma_t^2 = 0.5$ or $1$, the results also remain close to the nominal $\alpha$ level.  

For the scenarios where $f = 0.5$, inflation of the type I error rate only occurs when the values for $\sigma_t^2$ are small to moderate in combination with values for $\psi_h^2$ being small. Other combinations of true and hypothesized values for the nuisance parameters result in adequate to lower type I error rates compared to the nominal $\alpha$ level. When $f = 0.75$, all the scenarios lead to adequate and lower type I error rates. A decrease in the type I error rate can result in a reduction of power, but as the results in section \@ref(power) indicate, this is not an issue for the scenarios that are considered here. 

# Discussion {#discussion}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean ut elit odio. Donec fermentum tellus neque, vitae fringilla orci pretium vitae.























